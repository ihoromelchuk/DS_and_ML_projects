{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import tensorlayer \n",
    "import shutil\n",
    "import pathlib\n",
    "import skimage\n",
    "from skimage import transform \n",
    "from skimage.io import imread_collection\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread_collection\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert path to folder to store best model\n",
    "path_models = \"\" \n",
    "\n",
    "#Insert path to folder with test data\n",
    "path_images_train = \"\"\n",
    "\n",
    "#Insert path to folder with train data\n",
    "path_images_test = \"\"\n",
    "\n",
    "#Insert path to folder with new data\n",
    "path_images_new=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "def get_data(image_directory):\n",
    "    dirs = [d for d in os.listdir(image_directory) \n",
    "                   if os.path.isdir(os.path.join(image_directory, d))]\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in dirs:\n",
    "        label_directory = os.path.join(image_directory, d)\n",
    "        file_names = [os.path.join(label_directory, f)\n",
    "                      for f in os.listdir(label_directory)]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "train_image_directory = path_images_train\n",
    "test_image_directory = path_images_test\n",
    "\n",
    "train_images, train_labels = get_data(train_image_directory)\n",
    "test_images, test_labels = get_data(test_image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_existing_data(data):\n",
    "    if data[0][0].shape[-1]==3:\n",
    "        print('Converting image to black and white')\n",
    "        data[0] = rgb2gray(data)\n",
    "    images28 = [transform.resize(image, (28, 28)) for image in data[0]]\n",
    "    images28 = np.array(images28)\n",
    "    images28 = np.stack([i.flatten() for i in images28], axis=0)\n",
    "    labels_one_hot=np.eye(len(set(data[1])))[data[1]]\n",
    "    order = np.arange(images28.shape[0])\n",
    "    np.random.shuffle(order)\n",
    "    return images28[order], labels_one_hot[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_new_data(image_array):\n",
    "    new_images = []\n",
    "    for image in image_array:\n",
    "        if image.shape[-1]==3:\n",
    "            print('Converting image to black and white')\n",
    "            new_images.append(rgb2gray(image))\n",
    "    images28 = [transform.resize(image, (28, 28)) for image in new_images]\n",
    "    images28 = np.array(images28)\n",
    "    images28 = np.stack([i.flatten() for i in images28], axis=0)\n",
    "    return images28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, labels_train = transform_existing_data([train_images, train_labels])\n",
    "images_test, labels_test = transform_existing_data([test_images, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and training Neural Network\n",
    "\n",
    "def set_weights(var, shape):\n",
    "    initializer = tf.initializers.variance_scaling(scale=2.0, mode='fan_in', distribution='truncated_normal',\n",
    "                                             seed=None, dtype=tf.float32) \n",
    "    return tf.get_variable(var, shape=shape, initializer=initializer)\n",
    "\n",
    "def bias_element(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def convolutional_layer2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def pooling_layer_2_on_2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "y_= tf.placeholder(tf.float32, [None, 10])\n",
    "x_img = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "#Layer №1\n",
    "W_convolutional_L1 = set_weights(\"W1\", [5,5,1,32])\n",
    "b_convolutional_L1 = bias_element([32])\n",
    "h_convolutional_L1 = tf.nn.relu(convolutional_layer2d(x_img, W_convolutional_L1) + b_convolutional_L1)\n",
    "h_pool1 = pooling_layer_2_on_2(h_convolutional_L1)\n",
    "\n",
    "#Layer №2\n",
    "W_convolutional_L2 = set_weights(\"W2\", [5,5,32,64])\n",
    "b_convolutional_L2 = bias_element([64])\n",
    "h_convolutional_L2 = tf.nn.relu(convolutional_layer2d(h_pool1, W_convolutional_L2) + b_convolutional_L2)\n",
    "h_pool2 = pooling_layer_2_on_2(h_convolutional_L2)\n",
    "\n",
    "#Layer №3\n",
    "W_full_connection1 = set_weights(\"W3\", [7*7*64, 1024])\n",
    "b_full_connection1 = bias_element([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*64])\n",
    "h_full_connection1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_full_connection1) + b_full_connection1)\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "dropout = tf.nn.dropout(h_full_connection1, keep_prob)\n",
    "\n",
    "#Layer №4\n",
    "W_full_connection2 = set_weights(\"W5\", [1024, 10])\n",
    "b_full_connection2 = bias_element([10])\n",
    "y_conv = tf.matmul(dropout, W_full_connection2) + b_full_connection2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ihoromelchuk/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/ihoromelchuk/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Optimizers and parameters choice\n",
    "global All_optimizers \n",
    "All_optimizers= [(tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy),'SGD'),\n",
    "(tf.train.MomentumOptimizer(0.001, momentum=0.9, use_nesterov=False).minimize(cross_entropy),'Momentum'),\n",
    "(tf.train.MomentumOptimizer(0.001, momentum=0.9, use_nesterov=True).minimize(cross_entropy),'Nesterov_Momentum'),\n",
    "(tf.train.RMSPropOptimizer(0.001).minimize(cross_entropy),'RMSprop'),\n",
    "(tf.train.AdamOptimizer(0.001).minimize(cross_entropy),'Adam'),\n",
    "(tf.train.AdagradOptimizer(0.001).minimize(cross_entropy),'Adagrad'),\n",
    "(tf.train.AdadeltaOptimizer(learning_rate = 1).minimize(cross_entropy),'Adadelta'),\n",
    "(tensorlayer.optimizers.AMSGrad(0.001).minimize(cross_entropy),'AMSgrad')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs and batch size choice\n",
    "training_epochs = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN Training process\n",
    "def Neural_network_learning(Name_str, optimizer,train_data, train_labels, test_data, test_labels,initializer=tf.global_variables_initializer(),accur_metric=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)), tf.float32)), path=path_models):\n",
    "    with tf.Session() as Learning_process:\n",
    "        Learning_process.run(initializer)\n",
    "\n",
    "        print(\"\\nConducting Neural network learning process using optimizer \",Name_str )\n",
    "        avg_loss = list()\n",
    "        for epoch in range(1,training_epochs+1):\n",
    "            for batch in range(0,int(np.floor(train_data.shape[0]/batch_size))):\n",
    "                batch_xs, batch_ys = train_data[batch*batch_size:(batch*batch_size+batch_size)], train_labels[batch*batch_size:(batch*batch_size+batch_size)]\n",
    "                _, loss = Learning_process.run([optimizer, cross_entropy], \n",
    "                                    feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "                avg_loss.append(loss)\n",
    "    \n",
    "            train_accuracy = accur_metric.eval(feed_dict={x:batch_xs, y_:batch_ys, keep_prob: 1.0})\n",
    "            print(\"Epoch %03d, accuracy %.2f\"%(epoch, train_accuracy))\n",
    "\n",
    "        Accuracy = accur_metric.eval(feed_dict={x: test_data, y_: test_labels, keep_prob: 1.0})\n",
    "        print(\"Image classification accuracy on test data - %g\" %Accuracy)\n",
    "        save_path = tf.train.Saver().save(Learning_process,\"\".join([path,Name_str,\"/model.ckpt\"]))\n",
    "        return (Name_str,Accuracy,avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Avg_accuracy_and_loss_algorithms = dict()\n",
    "best_algorithm = All_optimizers[0][1]\n",
    "for i in range (0,len(All_optimizers)):\n",
    "    Avg_accuracy_and_loss_algorithms[All_optimizers[i][1]] = Neural_network_learning( All_optimizers[i][1], All_optimizers[i][0], images_train, labels_train, images_test, labels_test)\n",
    "    if i!=0:\n",
    "        if Avg_accuracy_and_loss_algorithms[best_algorithm][1] < Avg_accuracy_and_loss_algorithms[All_optimizers[i][1]][1]:\n",
    "            try:\n",
    "                shutil.rmtree(\"\".join([path_models,Avg_accuracy_and_loss_algorithms[best_algorithm][0]]))\n",
    "            except OSError as e:\n",
    "                print(\"Deletion error: %s : %s\" % (\"\".join([path_models,Avg_accuracy_and_loss_algorithms[best_algorithm][0]]), e.strerror))\n",
    "            best_algorithm = All_optimizers[i][1]\n",
    "        else:\n",
    "            try:\n",
    "                shutil.rmtree(\"\".join([path_models,All_optimizers[i][1]]))\n",
    "            except OSError as e:\n",
    "                print(Deletion error: %s : %s\" % (\"\".join([path_models,Avg_accuracy_and_loss_algorithms[best_algorithm][0]]), e.strerror))                   \n",
    "print(\"\\nBest algorithm - %s with accuracy of %.3f\"%(best_algorithm, Avg_accuracy_and_loss_algorithms[best_algorithm][1] ))\n",
    "shutil.move(\"\".join([path_models,best_algorithm]), \"\".join([path_models,\"Best_model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying new images\n",
    "\n",
    "file_names = [f for f in os.listdir(path_images_new) if '.DS_Store' not in f]\n",
    "new_data_directory = path_images_new\n",
    "new_directory = new_data_directory+'/*'\n",
    "new_images = imread_collection(new_directory)\n",
    "new_images = transform_new_data(new_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "with tf.Session() as Predict_session:\n",
    "    Predict_session.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().restore(Predict_session, \"\".join([path_models,'Best_model',\"/model.ckpt\"]))\n",
    "    predicted_labels=Predict_session.run(tf.argmax(y_conv,1),feed_dict={x: new_images, keep_prob: 1.0})\n",
    "print('Predicted classes:\\n')\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(\"Image \",file_names[i], \" – \",\"class '\",predicted_labels[i],\"'\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
